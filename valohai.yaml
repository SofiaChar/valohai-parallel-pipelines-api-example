- step:
    name: pipeline-task
    environment: aws-eu-west-1-g4dn-xlarge
    image: python:3.9
    command:
      - pip install requests
      - pip install valohai-utils
      - python parallel_pipelines_api.py {parameters}
    parameters:
      - name: harbours
        default: [harbor_A, harbor_B]
        type: string
        multiple: separate
        multiple-separator: ','
      - name: epochs
        default: [2, 5]
        multiple: separate
        multiple-separator: ","
        type: integer

- step:
    name: predict-models
    image: docker.io/noorai/dynamic-pipelines-demo:0.1
    environment: aws-eu-west-1-g4dn-xlarge
    command:
      - python predict.py {parameters}
    inputs:
      - name: dataset
        default:
          - dataset://harbor_A_test/latest
          - dataset://harbor_B_test/latest
    parameters:
      - name: exec_ids
        multiple: separate
        multiple-separator: ','
        optional: true
      - name: harbours
        type: string
        multiple: separate
        multiple-separator: ','

- step:
    name: preprocess-dataset
    image: docker.io/noorai/dynamic-pipelines-demo:0.1
    environment: aws-eu-west-1-g4dn-xlarge
    command:
      - python ./preprocess.py
    inputs:
      - name: dataset
        default: s3://valohai-demo-library-data/dynamic-pipelines/train/images.zip
      - name: labels
        default: s3://valohai-demo-library-data/dynamic-pipelines/train/*.csv
    parameters:
      - name: dataset_name
        default: harbor_A
      - name: validation_split
        default: 0.3
        type: float

- step:
    name: train
    image: docker.io/noorai/dynamic-pipelines-demo:0.1
    environment: aws-eu-west-1-g4dn-xlarge
    command:
      - python ./train_model.py {parameters}
    parameters:
      - name: epochs
        default: 25
        type: integer
      - name: learning_rate
        default: 0.001
        type: float
      - name: batch_size
        default: 64
        type: integer
      - name: dataset_name
        default: harbor_A
    inputs:
      - name: dataset
        default: dataset://{parameter:dataset_name}_train/latest
        optional: true

- pipeline:
    name: training-pipeline
    nodes:
      - name: preprocess
        type: execution
        step: preprocess-dataset
      - name: train
        type: execution
        step: train
    edges:
      - [preprocess.metadata.run_training_for, train.parameter.dataset_name]

